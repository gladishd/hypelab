---
title: "twitch_data"
author: "Dean"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(qdap)
library(dplyr)
library(tm)
library(wordcloud)
library(plotrix)
library(dendextend)
library(ggplot2)
library(ggthemes)
library(RWeka)
library(readr)
library(devtools)
library(stringr)
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(tidyr)
library(widyr)
library(gutenbergr)
library(igraph)
library(ggraph)
library(igraph)

```

## HypeLab (make sure to refresh after creating new folders)

```{r hypelab}
library(readr)
#Sometimes you may want to use another directory as the working directory. The usual way to change the working directory is setwd(), but please note that setwd() is not persistent in R Markdown (or other types of knitr source documents), which means setwd() only works for the current code chunk, and the working directory will be restored after this code chunk has been evaluated.


# Thank you for putting the content_id on there!  It's so easy to figure out which VoD each row refers to!  Now I can merge all files together with ease.  
setwd("C:/Users/gladi/Downloads/HypeLab/hypelab-main/rmd_files")
X732422168 <- read_csv("HypeLab/chat_data/732422168.csv")
highlights <- read_csv("HypeLab/highlights.csv")

```

```{r}
setwd("C:/Users/gladi/Downloads/HypeLab/hypelab-main/rmd_files")
X732590883 <- read_csv("HypeLab/chat_data/732590883.csv")
X732649888 <- read_csv("HypeLab/chat_data/732649888.csv")
X732902745 <- read_csv("HypeLab/chat_data/732902745.csv")
X732923687 <- read_csv("HypeLab/chat_data/732923687.csv")
X733055333 <- read_csv("HypeLab/chat_data/733055333.csv")
X733141847 <- read_csv("HypeLab/chat_data/733141847.csv")
X733218074 <- read_csv("HypeLab/chat_data/733218074.csv")
X733309525 <- read_csv("HypeLab/chat_data/733309525.csv")
X733381724 <- read_csv("HypeLab/chat_data/733381724.csv")
X733423912 <- read_csv("HypeLab/chat_data/733423912.csv")
X733560822 <- read_csv("HypeLab/chat_data/733560822.csv")
X733617744 <- read_csv("HypeLab/chat_data/733617744.csv")
X733778047 <- read_csv("HypeLab/chat_data/733778047.csv")
X733888858 <- read_csv("HypeLab/chat_data/733888858.csv")
X733897215 <- read_csv("HypeLab/chat_data/733897215.csv")
X733961959 <- read_csv("HypeLab/chat_data/733961959.csv")
X734080945 <- read_csv("HypeLab/chat_data/734080945.csv")
X734084727 <- read_csv("HypeLab/chat_data/734084727.csv")
X734149980 <- read_csv("HypeLab/chat_data/734149980.csv")
X734152320 <- read_csv("HypeLab/chat_data/734152320.csv")
X734297975 <- read_csv("HypeLab/chat_data/734297975.csv")
X734462011 <- read_csv("HypeLab/chat_data/734462011.csv")
X734509535 <- read_csv("HypeLab/chat_data/734509535.csv")
X734515241 <- read_csv("HypeLab/chat_data/734515241.csv")
X734599846 <- read_csv("HypeLab/chat_data/734599846.csv")
X734634839 <- read_csv("HypeLab/chat_data/734634839.csv")
X734642382 <- read_csv("HypeLab/chat_data/734642382.csv")
X734658719 <- read_csv("HypeLab/chat_data/734658719.csv")
X734658912 <- read_csv("HypeLab/chat_data/734658912.csv")
X734659884 <- read_csv("HypeLab/chat_data/734659884.csv")
X734712546 <- read_csv("HypeLab/chat_data/734712546.csv")
X734723250 <- read_csv("HypeLab/chat_data/734723250.csv")
X734813973 <- read_csv("HypeLab/chat_data/734813973.csv")
X734863301 <- read_csv("HypeLab/chat_data/734863301.csv")
X735037542 <- read_csv("HypeLab/chat_data/735037542.csv")
X735069020 <- read_csv("HypeLab/chat_data/735069020.csv")
X735098869 <- read_csv("HypeLab/chat_data/735098869.csv")
X735138477 <- read_csv("HypeLab/chat_data/735138477.csv")
X735238359 <- read_csv("HypeLab/chat_data/735238359.csv")
X735324021 <- read_csv("HypeLab/chat_data/735324021.csv")
X735365724 <- read_csv("HypeLab/chat_data/735365724.csv")
X735388275 <- read_csv("HypeLab/chat_data/735388275.csv")
X735406816 <- read_csv("HypeLab/chat_data/735406816.csv")
X735431204 <- read_csv("HypeLab/chat_data/735431204.csv")
X735446517 <- read_csv("HypeLab/chat_data/735446517.csv")
X735501370 <- read_csv("HypeLab/chat_data/735501370.csv")
X735573553 <- read_csv("HypeLab/chat_data/735573553.csv")
X735627516 <- read_csv("HypeLab/chat_data/735627516.csv")
X735681997 <- read_csv("HypeLab/chat_data/735681997.csv")
X735760878 <- read_csv("HypeLab/chat_data/735760878.csv")
X735802228 <- read_csv("HypeLab/chat_data/735802228.csv")
X735874898 <- read_csv("HypeLab/chat_data/735874898.csv")
X735888527 <- read_csv("HypeLab/chat_data/735888527.csv")
X735966185 <- read_csv("HypeLab/chat_data/735966185.csv")
X736060712 <- read_csv("HypeLab/chat_data/736060712.csv")
X736078601 <- read_csv("HypeLab/chat_data/736078601.csv")
X736081921 <- read_csv("HypeLab/chat_data/736081921.csv")
X736083431 <- read_csv("HypeLab/chat_data/736083431.csv")
X736099235 <- read_csv("HypeLab/chat_data/736099235.csv")
X736114714 <- read_csv("HypeLab/chat_data/736114714.csv")
X736275158 <- read_csv("HypeLab/chat_data/736275158.csv")
X736298400 <- read_csv("HypeLab/chat_data/736298400.csv")
X738399552 <- read_csv("HypeLab/chat_data/738399552.csv")
X738453831 <- read_csv("HypeLab/chat_data/738453831.csv")
X738603474 <- read_csv("HypeLab/chat_data/738603474.csv")
X738704813 <- read_csv("HypeLab/chat_data/738704813.csv")
X738830532 <- read_csv("HypeLab/chat_data/738830532.csv")
X738867256 <- read_csv("HypeLab/chat_data/738867256.csv")
X738995313 <- read_csv("HypeLab/chat_data/738995313.csv")
X739077387 <- read_csv("HypeLab/chat_data/739077387.csv")
X739134629 <- read_csv("HypeLab/chat_data/739134629.csv")
X739285759 <- read_csv("HypeLab/chat_data/739285759.csv")
X739300735 <- read_csv("HypeLab/chat_data/739300735.csv")
X739353870 <- read_csv("HypeLab/chat_data/739353870.csv")
X739377281 <- read_csv("HypeLab/chat_data/739377281.csv")
X739435110 <- read_csv("HypeLab/chat_data/739435110.csv")
X739493901 <- read_csv("HypeLab/chat_data/739493901.csv")
X739673683 <- read_csv("HypeLab/chat_data/739673683.csv")
X739861214 <- read_csv("HypeLab/chat_data/739861214.csv")
X739993282 <- read_csv("HypeLab/chat_data/739993282.csv")
X740012400 <- read_csv("HypeLab/chat_data/740012400.csv")
X740016439 <- read_csv("HypeLab/chat_data/740016439.csv")
X740254008 <- read_csv("HypeLab/chat_data/740254008.csv")
X740456803 <- read_csv("HypeLab/chat_data/740456803.csv")
X740577349 <- read_csv("HypeLab/chat_data/740577349.csv")
X740610742 <- read_csv("HypeLab/chat_data/740610742.csv")
X740650633 <- read_csv("HypeLab/chat_data/740650633.csv")
X740688070 <- read_csv("HypeLab/chat_data/740688070.csv")
X740696504 <- read_csv("HypeLab/chat_data/740696504.csv")
X740791246 <- read_csv("HypeLab/chat_data/740791246.csv")
X740897151 <- read_csv("HypeLab/chat_data/740897151.csv")
X740929502 <- read_csv("HypeLab/chat_data/740929502.csv")
X741084619 <- read_csv("HypeLab/chat_data/741084619.csv")
X741112115 <- read_csv("HypeLab/chat_data/741112115.csv")
X741195689 <- read_csv("HypeLab/chat_data/741195689.csv")
X741218165 <- read_csv("HypeLab/chat_data/741218165.csv")
X741269486 <- read_csv("HypeLab/chat_data/741269486.csv")
X741276168 <- read_csv("HypeLab/chat_data/741276168.csv")
X741302422 <- read_csv("HypeLab/chat_data/741302422.csv")
X741407576 <- read_csv("HypeLab/chat_data/741407576.csv")
X741471634 <- read_csv("HypeLab/chat_data/741471634.csv")
X741588256 <- read_csv("HypeLab/chat_data/741588256.csv")
X741589911 <- read_csv("HypeLab/chat_data/741589911.csv")
X741627098 <- read_csv("HypeLab/chat_data/741627098.csv")
X741643559 <- read_csv("HypeLab/chat_data/741643559.csv")
X741661485 <- read_csv("HypeLab/chat_data/741661485.csv")
X741699801 <- read_csv("HypeLab/chat_data/741699801.csv")
X741748646 <- read_csv("HypeLab/chat_data/741748646.csv")
X741785910 <- read_csv("HypeLab/chat_data/741785910.csv")
X741820994 <- read_csv("HypeLab/chat_data/741820994.csv")
X741835950 <- read_csv("HypeLab/chat_data/741835950.csv")
X741873832 <- read_csv("HypeLab/chat_data/741873832.csv")
X741941851 <- read_csv("HypeLab/chat_data/741941851.csv")
X741953896 <- read_csv("HypeLab/chat_data/741953896.csv")
X742037638 <- read_csv("HypeLab/chat_data/742037638.csv")
X742539532 <- read_csv("HypeLab/chat_data/742539532.csv")
X742547922 <- read_csv("HypeLab/chat_data/742547922.csv")
X742641837 <- read_csv("HypeLab/chat_data/742641837.csv")
X742729577 <- read_csv("HypeLab/chat_data/742729577.csv")
X742802990 <- read_csv("HypeLab/chat_data/742802990.csv")
X742803758 <- read_csv("HypeLab/chat_data/742803758.csv")
X742856138 <- read_csv("HypeLab/chat_data/742856138.csv")
X743058030 <- read_csv("HypeLab/chat_data/743058030.csv")
X743108790 <- read_csv("HypeLab/chat_data/743108790.csv")
X743165820 <- read_csv("HypeLab/chat_data/743165820.csv")
X743197021 <- read_csv("HypeLab/chat_data/743197021.csv")
X743278970 <- read_csv("HypeLab/chat_data/743278970.csv")
X743535931 <- read_csv("HypeLab/chat_data/743535931.csv")
X743642806 <- read_csv("HypeLab/chat_data/743642806.csv")
X743683798 <- read_csv("HypeLab/chat_data/743683798.csv")
X743706675 <- read_csv("HypeLab/chat_data/743706675.csv")
X743750639 <- read_csv("HypeLab/chat_data/743750639.csv")
X743826825 <- read_csv("HypeLab/chat_data/743826825.csv")
X744000098 <- read_csv("HypeLab/chat_data/744000098.csv")
X744716456 <- read_csv("HypeLab/chat_data/744716456.csv")
X744749795 <- read_csv("HypeLab/chat_data/744749795.csv")
X744820200 <- read_csv("HypeLab/chat_data/744820200.csv")
X744863077 <- read_csv("HypeLab/chat_data/744863077.csv")
X744978950 <- read_csv("HypeLab/chat_data/744978950.csv")
X744986081 <- read_csv("HypeLab/chat_data/744986081.csv")
X745021335 <- read_csv("HypeLab/chat_data/745021335.csv")
X745033438 <- read_csv("HypeLab/chat_data/745033438.csv")
X745064355 <- read_csv("HypeLab/chat_data/745064355.csv")
X745095778 <- read_csv("HypeLab/chat_data/745095778.csv")
X745103945 <- read_csv("HypeLab/chat_data/745103945.csv")
X745104227 <- read_csv("HypeLab/chat_data/745104227.csv")
X745202461 <- read_csv("HypeLab/chat_data/745202461.csv")
X745229298 <- read_csv("HypeLab/chat_data/745229298.csv")
X745532793 <- read_csv("HypeLab/chat_data/745532793.csv")
X745586635 <- read_csv("HypeLab/chat_data/745586635.csv")
X745724664 <- read_csv("HypeLab/chat_data/745724664.csv")
X745725181 <- read_csv("HypeLab/chat_data/745725181.csv")
X745794265 <- read_csv("HypeLab/chat_data/745794265.csv")
X745894887 <- read_csv("HypeLab/chat_data/745894887.csv")
X745916093 <- read_csv("HypeLab/chat_data/745916093.csv")
X745979816 <- read_csv("HypeLab/chat_data/745979816.csv")
X746008884 <- read_csv("HypeLab/chat_data/746008884.csv")
X746017979 <- read_csv("HypeLab/chat_data/746017979.csv")
X746067139 <- read_csv("HypeLab/chat_data/746067139.csv")
X746192173 <- read_csv("HypeLab/chat_data/746192173.csv")
X746237660 <- read_csv("HypeLab/chat_data/746237660.csv")
X746354761 <- read_csv("HypeLab/chat_data/746354761.csv")
X746543309 <- read_csv("HypeLab/chat_data/746543309.csv")
X746605777 <- read_csv("HypeLab/chat_data/746605777.csv")
X746694425 <- read_csv("HypeLab/chat_data/746694425.csv")
X746812421 <- read_csv("HypeLab/chat_data/746812421.csv")
X746812792 <- read_csv("HypeLab/chat_data/746812792.csv")
X748857757 <- read_csv("HypeLab/chat_data/748857757.csv")
X748862134 <- read_csv("HypeLab/chat_data/748862134.csv")
X748886983 <- read_csv("HypeLab/chat_data/748886983.csv")
X749043488 <- read_csv("HypeLab/chat_data/749043488.csv")
X749133787 <- read_csv("HypeLab/chat_data/749133787.csv")
X749168536 <- read_csv("HypeLab/chat_data/749168536.csv")
X749217129 <- read_csv("HypeLab/chat_data/749217129.csv")
X749385237 <- read_csv("HypeLab/chat_data/749385237.csv")
X749480811 <- read_csv("HypeLab/chat_data/749480811.csv")
X749563932 <- read_csv("HypeLab/chat_data/749563932.csv")
X749572538 <- read_csv("HypeLab/chat_data/749572538.csv")
X749626536 <- read_csv("HypeLab/chat_data/749626536.csv")
X749706053 <- read_csv("HypeLab/chat_data/749706053.csv")
X749709633 <- read_csv("HypeLab/chat_data/749709633.csv")
X749771125 <- read_csv("HypeLab/chat_data/749771125.csv")
X749801009 <- read_csv("HypeLab/chat_data/749801009.csv")
X749906356 <- read_csv("HypeLab/chat_data/749906356.csv")
X749996120 <- read_csv("HypeLab/chat_data/749996120.csv")
X749998158 <- read_csv("HypeLab/chat_data/749998158.csv")
X750082337 <- read_csv("HypeLab/chat_data/750082337.csv")
X750117402 <- read_csv("HypeLab/chat_data/750117402.csv")
X750152941 <- read_csv("HypeLab/chat_data/750152941.csv")
X750245426 <- read_csv("HypeLab/chat_data/750245426.csv")
X750561643 <- read_csv("HypeLab/chat_data/750561643.csv")
X751921888 <- read_csv("HypeLab/chat_data/751921888.csv")
X751985019 <- read_csv("HypeLab/chat_data/751985019.csv")
X752038852 <- read_csv("HypeLab/chat_data/752038852.csv")
X752149221 <- read_csv("HypeLab/chat_data/752149221.csv")
X752212664 <- read_csv("HypeLab/chat_data/752212664.csv")
X752714338 <- read_csv("HypeLab/chat_data/752714338.csv")
X752730210 <- read_csv("HypeLab/chat_data/752730210.csv")
X752770512 <- read_csv("HypeLab/chat_data/752770512.csv")
X752781099 <- read_csv("HypeLab/chat_data/752781099.csv")
X752848796 <- read_csv("HypeLab/chat_data/752848796.csv")
X753029621 <- read_csv("HypeLab/chat_data/753029621.csv")
X753109461 <- read_csv("HypeLab/chat_data/753109461.csv")
X753175271 <- read_csv("HypeLab/chat_data/753175271.csv")
X753444544 <- read_csv("HypeLab/chat_data/753444544.csv")
X753631208 <- read_csv("HypeLab/chat_data/753631208.csv")
X753812846 <- read_csv("HypeLab/chat_data/753812846.csv")
X753884301 <- read_csv("HypeLab/chat_data/753884301.csv")
X755250632 <- read_csv("HypeLab/chat_data/755250632.csv")
X755360796 <- read_csv("HypeLab/chat_data/755360796.csv")
X755466433 <- read_csv("HypeLab/chat_data/755466433.csv")
X755761888 <- read_csv("HypeLab/chat_data/755761888.csv")
X756028285 <- read_csv("HypeLab/chat_data/756028285.csv")
X756339345 <- read_csv("HypeLab/chat_data/756339345.csv")
X756355377 <- read_csv("HypeLab/chat_data/756355377.csv")
X756381896 <- read_csv("HypeLab/chat_data/756381896.csv")
X756467809 <- read_csv("HypeLab/chat_data/756467809.csv")
X756764211 <- read_csv("HypeLab/chat_data/756764211.csv")
X756867423 <- read_csv("HypeLab/chat_data/756867423.csv")



```


```{r makeAggregate}
#setwd("C:/Users/gladi/Downloads/HypeLab/chat_data")

#for (data in list.files()){
#  
#  # Create the first data if no data exist yet
#  if (!exists("dataset")){
#    dataset <- read.csv(data, header=TRUE)
#  }
#  
#  # if data already exist, then append it together
#  if (exists("dataset")){
#    tempory <-read.csv(data, header=TRUE)
#    dataset <-unique(rbind(dataset, tempory))
#    rm(tempory)
#  }
#}

# much quicker
library(plyr)
dataset <- ldply(list.files(), read.csv, header=TRUE)

View(dataset)

#View(X732422168$message)
```

## Plots

```{r plots, echo=FALSE}
# qdap is loaded

# Print new_text to the console
new_text <- "DataCamp is the first online learning platform that focuses on building the best learning experience specifically for Data Science. We have offices in Boston and Belgium and to date, we trained over 250,000 (aspiring) data scientists in over 150 countries. These data science enthusiasts completed more than 9 million exercises. You can take free beginner courses, or subscribe for $25/month to get access to all premium courses."

#new_text_split <- strsplit(new_text, ' ')

# Find the 10 most frequent terms: term_count
term_count <- freq_terms(new_text, 10)
#term_count <- sort(table(new_text_split), decreasing=T)

# Plot term_count
plot(term_count)
```

```{r morePlots}
library(tidytext)
library(tidyverse)
library(rmarkdown)
```


```{r}
X732422168$message[1]
cat(X732422168$message[1])

s <- list(name="DataFlair", age=29, GPA = 4.0)

class(s) <- "student"

s

#library(jsonlite)



library(rjson)
library(stringr)
original <- "{'body': 'yoooooooooooo', 'fragments': [{'text': 'yoooooooooooo'}], 'is_action': False, 'user_color': '#0000FF', 'user_notice_params': {}}"
temp <- str_replace_all(original, "'", "\"")
temp <- str_replace_all(temp, "False", "\"False\"")
temp <- str_replace_all(temp, "True", "\"True\"")
converted <- fromJSON(temp)

```


```{r}
filterByValues = c(732422168, 732590883, 732649888, 732902745, 732923687, 733055333, 733141847, 733218074, 733309525, 733381724, 733423912, 733560822, 733617744, 733778047, 733888858, 733897215, 733961959, 734080945, 734084727, 734149980, 734152320, 734297975, 734462011, 734509535, 734515241, 734599846, 734634839, 734642382, 734658719, 734658912, 734659884, 734712546, 734723250, 734813973, 734863301, 735037542, 735069020, 735098869, 735138477, 735238359, 735324021, 735365724, 735388275, 735406816, 735431204, 735446517, 735501370, 735573553, 735627516, 735681997, 735760878, 735802228, 735874898, 735888527, 735966185, 736060712, 736078601, 736081921, 736083431, 736099235, 736114714, 736275158, 736298400, 738399552, 738453831, 738603474, 738704813, 738830532, 738867256, 738995313, 739077387, 739134629, 739285759, 739300735, 739353870, 739377281, 739435110, 739493901, 739673683, 739861214, 739993282, 740012400, 740016439, 740254008, 740456803, 740577349, 740610742, 740650633, 740688070, 740696504, 740791246, 740897151, 740929502, 741084619, 741112115, 741195689, 741218165, 741269486, 741276168, 741302422, 741407576, 741471634, 741588256, 741589911, 741627098, 741643559, 741661485, 741699801, 741748646, 741785910, 741820994, 741835950, 741873832, 741941851, 741953896, 742037638, 742539532, 742547922, 742641837, 742729577, 742802990, 742803758, 742856138, 743058030, 743108790, 743165820, 743197021, 743278970, 743535931, 743642806, 743683798, 743706675, 743750639, 743826825, 744000098, 744716456, 744749795, 744820200, 744863077, 744978950, 744986081, 745021335, 745033438, 745064355, 745095778, 745103945, 745104227, 745202461, 745229298, 745532793, 745586635, 745724664, 745725181, 745794265, 745894887, 745916093, 745979816, 746008884, 746017979, 746067139, 746192173, 746237660, 746354761, 746543309, 746605777, 746694425, 746812421, 746812792, 748857757, 748862134, 748886983, 749043488, 749133787, 749168536, 749217129, 749385237, 749480811, 749563932, 749572538, 749626536, 749706053, 749709633, 749771125, 749801009, 749906356, 749996120, 749998158, 750082337, 750117402, 750152941, 750245426, 750561643, 751921888, 751985019, 752038852, 752149221, 752212664, 752714338, 752730210, 752770512, 752781099, 752848796, 753029621, 753109461, 753175271, 753444544, 753631208, 753812846, 753884301, 755250632, 755360796, 755466433, 755761888, 756028285, 756339345, 756355377, 756381896, 756467809, 756764211, 756867423)
length(filterByValues)
```



```{r runThisChunkForTheMainTest}
# possible prerequisite calculations:
unique_vod_id_values <- unique(highlights$vod_id)


filterByValues = c(732422168, 732590883, 732649888, 732902745, 732923687, 733055333, 733141847, 733218074, 733309525, 733381724, 733423912, 733560822, 733617744, 733778047, 733888858, 733897215, 733961959, 734080945, 734084727, 734149980, 734152320, 734297975, 734462011, 734509535, 734515241, 734599846, 734634839, 734642382, 734658719, 734658912, 734659884, 734712546, 734723250, 734813973, 734863301, 735037542, 735069020, 735098869, 735138477, 735238359, 735324021, 735365724, 735388275, 735406816, 735431204, 735446517, 735501370, 735573553, 735627516, 735681997, 735760878, 735802228, 735874898, 735888527, 735966185, 736060712, 736078601, 736081921, 736083431, 736099235, 736114714, 736275158, 736298400, 738399552, 738453831, 738603474, 738704813, 738830532, 738867256, 738995313, 739077387, 739134629, 739285759, 739300735, 739353870, 739377281, 739435110, 739493901, 739673683, 739861214, 739993282, 740012400, 740016439, 740254008, 740456803, 740577349, 740610742, 740650633, 740688070, 740696504, 740791246, 740897151, 740929502, 741084619, 741112115, 741195689, 741218165, 741269486, 741276168, 741302422, 741407576, 741471634, 741588256, 741589911, 741627098, 741643559, 741661485, 741699801, 741748646, 741785910, 741820994, 741835950, 741873832, 741941851, 741953896, 742037638, 742539532, 742547922, 742641837, 742729577, 742802990, 742803758, 742856138, 743058030, 743108790, 743165820, 743197021, 743278970, 743535931, 743642806, 743683798, 743706675, 743750639, 743826825, 744000098, 744716456, 744749795, 744820200, 744863077, 744978950, 744986081, 745021335, 745033438, 745064355, 745095778, 745103945, 745104227, 745202461, 745229298, 745532793, 745586635, 745724664, 745725181, 745794265, 745894887, 745916093, 745979816, 746008884, 746017979, 746067139, 746192173, 746237660, 746354761, 746543309, 746605777, 746694425, 746812421, 746812792, 748857757, 748862134, 748886983, 749043488, 749133787, 749168536, 749217129, 749385237, 749480811, 749563932, 749572538, 749626536, 749706053, 749709633, 749771125, 749801009, 749906356, 749996120, 749998158, 750082337, 750117402, 750152941, 750245426, 750561643, 751921888, 751985019, 752038852, 752149221, 752212664, 752714338, 752730210, 752770512, 752781099, 752848796, 753029621, 753109461, 753175271, 753444544, 753631208, 753812846, 753884301, 755250632, 755360796, 755466433, 755761888, 756028285, 756339345, 756355377, 756381896, 756467809, 756764211, 756867423)


#correspondingFileValues = c(X732422168, X732590883, X732649888, X732902745, X732923687, X733055333, X733141847, X733218074, X733309525, X733381724, X733423912, X733560822, X733617744, X733778047, X733888858, X733897215, X733961959, X734080945, X734084727, X734149980, X734152320, X734297975, X734462011, X734509535, X734515241, X734599846, X734634839, X734642382, X734658719, X734658912, X734659884, X734712546, X734723250, X734813973, X734863301, X735037542, X735069020, X735098869, X735138477, X735238359, X735324021, X735365724, X735388275, X735406816, X735431204, X735446517, X735501370, X735573553, X735627516, X735681997, X735760878, X735802228, X735874898, X735888527, X735966185, X736060712, X736078601, X736081921, X736083431, X736099235, X736114714, X736275158, X736298400, X738399552, X738453831, X738603474, X738704813, X738830532, X738867256, X738995313, X739077387, X739134629, X739285759, X739300735, X739353870, X739377281, X739435110, X739493901, X739673683, X739861214, X739993282, X740012400, X740016439, X740254008, X740456803, X740577349, X740610742, X740650633, X740688070, X740696504, X740791246, X740897151, X740929502, X741084619, X741112115, X741195689, X741218165, X741269486, X741276168, X741302422, X741407576, X741471634, X741588256, X741589911, X741627098, X741643559, X741661485, X741699801, X741748646, X741785910, X741820994, X741835950, X741873832, X741941851, X741953896, X742037638, X742539532, X742547922, X742641837, X742729577, X742802990, X742803758, X742856138, X743058030, X743108790, X743165820, X743197021, X743278970, X743535931, X743642806, X743683798, X743706675, X743750639, X743826825, X744000098, X744716456, X744749795, X744820200, X744863077, X744978950, X744986081, X745021335, X745033438, X745064355, X745095778, X745103945, X745104227, X745202461, X745229298, X745532793, X745586635, X745724664, X745725181, X745794265, X745894887, X745916093, X745979816, X746008884, X746017979, X746067139, X746192173, X746237660, X746354761, X746543309, X746605777, X746694425, X746812421, X746812792, X748857757, X748862134, X748886983, X749043488, X749133787, X749168536, X749217129, X749385237, X749480811, X749563932, X749572538, X749626536, X749706053, X749709633, X749771125, X749801009, X749906356, X749996120, X749998158, X750082337, X750117402, X750152941, X750245426, X750561643, X751921888, X751985019, X752038852, X752149221, X752212664, X752714338, X752730210, X752770512, X752781099, X752848796, X753029621, X753109461, X753175271, X753444544, X753631208, X753812846, X753884301, X755250632, X755360796, X755466433, X755761888, X756028285, X756339345, X756355377, X756381896, X756467809, X756764211, X756867423)

finalBucket1 <- vector()
finalBucket2 <- vector()

for (val in 1:length(filterByValues)) {

  # For each row of highlights: 
  # 1. Use the Timestamp and Length columns to calculate a specific range of time: 
  #         (timestamp, timestamp + length)
  highlights_modified <- highlights %>% 
    mutate(timestamp_plus_length = timestamp + length) %>% # we'll do the single case for now
    filter(vod_id == filterByValues[val]) # <--------------------------------change this to be for different video on demand IDs.
  corresponding_file <- eval(parse(text = paste('X', filterByValues[val], sep=''))) # <--------------------------------change this to be for different video on demand IDs.  
  #if (i == 1) { corresponding_file <- X732422168 }
  #if (i == 2) { corresponding_file <- X732590883 }
  #if (i == 3) { corresponding_file <- X732649888 }
  
  lower_bound1 <- highlights_modified$timestamp[1]
  upper_bound1 <- highlights_modified$timestamp_plus_length[1]
  
  lower_bound2 <- highlights_modified$timestamp[2]
  upper_bound2 <- highlights_modified$timestamp_plus_length[2]
  
  allBoundsNested <- vector()
  for (i in 1:nrow(highlights_modified)) {
    lowerBoundTemp <- highlights_modified$timestamp[i]
    upperBoundTemp <- highlights_modified$timestamp_plus_length[i]
    boundsVector <- c(lowerBoundTemp, upperBoundTemp)
    allBoundsNested <- c(allBoundsNested, boundsVector)
  }
  
  vod_id_value <- highlights_modified$vod_id[1]
  
  
  # 2. Take the vod_id value for that row, and find the corresponding file from chat_data.
  
  
  
  #         * For this file: if content_offset_seconds is within (timestamp, timestamp + length), 
  #                          then put that Message into bucket 1.  
  #                          otherwise, put that Message into bucket 2.  
  # Compare word frequency between bucket 1 and bucket 2
  
  
  # (corresponding_file$content_offset_seconds[2800] >= lower_bound) && (corresponding_file$content_offset_seconds[2800] <= upper_bound) returns TRUE
  
  
  library(rjson)
  library(stringr)
  
  
  bucket1 <- vector() # part of the highlight
  bucket2 <- vector() # not part of the highlight
  for (i in 1:nrow(corresponding_file)) { # if the content offset is within the bounds
    tryCatch({
      original <- corresponding_file$message[i]
      # Before making replacements to make *original* a valid JSON string, we need to 
      # protect the textual integrity of the value referred to by body and 
      # the value referred to by fragments/text, which is clearly derived from the body key.  
      
      #firstPart <- "{'body': '"
      #remainder <- sub(".*body': '", "", original) 
      #secondPart <- sub("', 'fragments.*", "", remainder)
      #thirdPart <- "', 'fragments': [{'text': '"
      #remainder <- sub(".*'text': '", "", original)
      #fourthPart <- sub("'}], 'is_action'.*", "", remainder)
      #lastPart <- paste("'}], 'is_action': ", sub(".*'}], 'is_action': ", "", original), sep="")
      
      #firstPart, thirdPart, lastPart
      #firstPart <- str_replace_all(firstPart, "'", "\"")
      #firstPart <- str_replace_all(firstPart, "False", "\"False\"")
      #firstPart <- str_replace_all(firstPart, "True", "\"True\"")
      
      #thirdPart <- str_replace_all(thirdPart, "'", "\"")
      #thirdPart <- str_replace_all(thirdPart, "False", "\"False\"")
      #thirdPart <- str_replace_all(thirdPart, "True", "\"True\"")
      
      #lastPart <- str_replace_all(lastPart, "'", "\"")
      #lastPart <- str_replace_all(lastPart, "False", "\"False\"")
      #lastPart <- str_replace_all(lastPart, "True", "\"True\"")
      
      #temp <- paste(firstPart, secondPart, thirdPart, fourthPart, lastPart, sep="")
      
      #converted <- fromJSON(temp)
      
      
      
      remainder <- sub(".*body': '", "", original) 
      textBody <- sub("', '.*", "", remainder)
      
      textBody <- sub(".*body': \"", "", textBody) # a minority of entries come in a different format, with JSON-like backslashes
      textBody <- sub("\".*", "", textBody)
      
      
      
      #isHighlight <- FALSE
      #for (j in 1:length(allBoundsNested)) {
      #  if (j %% 2 == 1) { # if it's an odd index
      #    if (
      #      dataset$content_offset_seconds[j] >= allBoundsNested[j]
      #      && 
      #      dataset$content_offset_seconds[j + 1] <= allBoundsNested[j + 1]
      #    ) {
      #      isHighlight <- TRUE
      #      }
      #  }
      #}
      
      messageInHighlight <- FALSE
      for(n in 0:(floor(length(allBoundsNested)/2) - 1)) {
        if (allBoundsNested[2*n + 1] <= corresponding_file$content_offset_seconds[i] && 
            corresponding_file$content_offset_seconds[i] <= allBoundsNested[2*n + 2]) {
          messageInHighlight <- TRUE
        }
      }
  
      if ( messageInHighlight
        #if the message we're looking at occurs during any kind of highlight
        ) {
      # if (isHighlight) {
        bucket1 <- c(bucket1, paste(textBody, filterByValues[val], sep=""))
      } else {
        bucket2 <- c(bucket2, paste(textBody, filterByValues[val], sep=""))
      }
    }, error=function(e){cat("ERROR On ROW ", i, ":",conditionMessage(e), "\n")})
  }
  
  finalBucket1 = c(finalBucket1, bucket1)
  finalBucket2 = c(finalBucket2, bucket2)

}

# Last step is scaling this not just for one comparison but for ALL ROWS IN highlights <-> ALL FILES in chat_data
# Figure out what each column means, as well as the key:value pairs within the message column for each chat_data file
bucket1 <- finalBucket1 # length for video file 1 is 340
bucket2 <- finalBucket2 # length for video file 2 is 11845

write(bucket1, 'bucket1Original.txt')
write(bucket2, 'bucket2Original.txt')
```

```{r}
bucket1 <- finalBucket1 # length for video file 1 is 340
bucket2 <- finalBucket2 # length for video file 2 is 11845

bucket1 <- lapply(bucket1, function(s) strsplit(s, ' ')) # atomize words
bucket1 <- unlist(bucket1) # flatten

bucket2 <- lapply(bucket2, function(s) strsplit(s, ' ')) # atomize words
bucket2 <- unlist(bucket2) # flatten

```

```{r}
# Write to file
write(bucket1, 'bucket1.txt')
write(bucket2, 'bucket2.txt')

# Remove stop words
bucket1 <- rm_stopwords(bucket1, tm::stopwords("english"))
bucket1 <- bucket1[bucket1 != 'character(0)']
bucket2 <- rm_stopwords(bucket2, tm::stopwords("english"))
bucket2 <- bucket2[bucket2 != 'character(0)']

```

```{r}

# Find the 10 most frequent terms: term_count
term_count_during_any_highlight <- freq_terms(bucket1, 20)
term_count_when_no_highlight <- freq_terms(bucket2, 20)

# Plot term_count
plot(term_count_during_any_highlight)
plot(term_count_when_no_highlight)
```

```{r}
# reduce vector sizes
bucket1 <- sample(bucket1, 30000, replace = FALSE)

# Make a vector source
bucket1_source <- VectorSource(bucket1)

# Make a volatile corpus
bucket1_corpus <- VCorpus(bucket1_source)

# Clean the corpus
#clean_corpus <- function(corpus){
#  corpus <- tm_map(corpus, stripWhitespace)
#  corpus <- tm_map(corpus, removePunctuation)
#  corpus <- tm_map(corpus, content_transformer(tolower))
#  corpus <- tm_map(corpus, removeWords, stopwords("en"))
#  return(corpus)
#}

#bucket1_clean_corp <- clean_corpus(bucket1_corpus)
bucket1_clean_corp <- bucket1_corpus


# Convert TDM to matrix
bucket1_tdm <- TermDocumentMatrix(bucket1_clean_corp)
bucket1_m <- as.matrix(bucket1_tdm)

# Sum rows and frequency data frame
bucket1_term_freq <- rowSums(bucket1_m)

head(bucket1_term_freq)

bucket1_word_freqs <- data.frame(
  term = names(bucket1_term_freq),
  num = bucket1_term_freq
)

head(bucket1_word_freqs)
```

```{r}
# Create purple_orange
brown_blue_green <- brewer.pal(10, "BrBG")

# Drop 2 faintest colors
## In this case the faintest colors are in the middle as it fades from orange to purple

brown_blue_green <- brown_blue_green[-(5:6)]


# The wordcloud package is loaded

# Create a wordcloud for the values in word_freqs
wordcloud(bucket1_word_freqs$term, bucket1_word_freqs$num,
  max.words = 100, colors=brown_blue_green)

```

```{r}
# Reduce vector size 
bucket2 <- sample(bucket2, 17000, replace = FALSE)

# Make a vector source
bucket2_source <- VectorSource(bucket2)

# Make a volatile corpus
bucket2_corpus <- VCorpus(bucket2_source)

# Clean the corpus
#clean_corpus <- function(corpus){
#  corpus <- tm_map(corpus, stripWhitespace)
#  corpus <- tm_map(corpus, removePunctuation)
#  corpus <- tm_map(corpus, content_transformer(tolower))
#  corpus <- tm_map(corpus, removeWords, stopwords("en"))
#  return(corpus)
#}

#bucket2_clean_corp <- clean_corpus(bucket2_corpus)
bucket2_clean_corp <- bucket2_corpus

# Convert TDM to matrix
bucket2_tdm <- TermDocumentMatrix(bucket2_clean_corp)
bucket2_m <- as.matrix(bucket2_tdm)

# Sum rows and frequency data frame
bucket2_term_freq <- rowSums(bucket2_m)

head(bucket2_term_freq)

bucket2_word_freqs <- data.frame(
  term = names(bucket2_term_freq),
  num = bucket2_term_freq
)

head(bucket2_word_freqs)

```


```{r}
# Create purple_orange
brown_blue_green <- brewer.pal(10, "BrBG")

# Drop 2 faintest colors
## In this case the faintest colors are in the middle as it fades from orange to purple

brown_blue_green <- brown_blue_green[-(5:6)]

# The wordcloud package is loaded

# Create a wordcloud for the values in word_freqs
wordcloud(bucket2_word_freqs$term, bucket2_word_freqs$num,
  max.words = 100, colors=brown_blue_green)

```


```{r commonWords}
# Combine both corpora: all_tweets
all_coffee <- paste(bucket1, collapse = "")
all_chardonnay <- paste(bucket2, collapse = "")
all_tweets <- c(all_coffee, all_chardonnay)

# clean all_tweets
all_tweets <- VectorSource(all_tweets)
all_corpus <- VCorpus(all_tweets)



# Create all_tdm
all_tdm <- TermDocumentMatrix(all_corpus)

# Give the columns distinct names
colnames(all_tdm) <- c("during highlight", "otherwise")

# Create all_m
all_m <- as.matrix(all_tdm)

# Create comparison cloud
comparison.cloud(all_m,
                 colors = c("orange", "blue"),
                 max.words = 50)

```

```{r polarizedTagCloud}
# Identify terms shared by both documents
common_words <- subset(
  all_m,
  all_m[, 1] > 0 & all_m[, 2] > 0
)

head(common_words)

# calc common words and difference
difference <- abs(common_words[, 1] - common_words[, 2])
common_words <- cbind(common_words, difference)
common_words <- common_words[order(common_words[, 3],
                                   decreasing = T), ]
head(common_words)


top25_df <- data.frame(x = common_words[1:25, 1],
                       y = common_words[1:25, 2],
                       labels = rownames(common_words[1:25, ]))

# The plotrix package has been loaded

# Make pyramid plot
pyramid.plot(top25_df$x, top25_df$y,
             labels = top25_df$labels, 
             main = "Words in Common",
             gap = 228,
             laxlab = NULL,
             raxlab = NULL, 
             unit = NULL,
             top.labels = c("During Highlights",
                            "Words",
                            "Otherwise")
             )

```


```{r wordAssociations}
#word_associate(
 # bucket2_word_freqs,
 # match.string = c("kekw"),
  #stopwords = c(Top200Words, "coffee", "amp"),
 # network.plot = T,
#  cloud.colors = c("gray85", "darkred")
 # )
```

```{r}
bucket1highlights <- read_csv("bucket1-highlights.txt")
bucket2nohighlight <- read_csv("bucket2-no-highlight.txt")



```


```{r}
# This block of code is just for doing some preliminary bigram stuff
bucket1Original <- read_csv("bucket1Original.txt")

bucket2Original <- read_csv("bucket2Original.txt")


bucket1asVector <- unlist(bucket1Original$amateurs732422168)
bucket1asVector <- iconv(enc2utf8(bucket1asVector), sub='byte')



sentences <- lapply(bucket1asVector, function(x) { str_sub(x, 1, nchar(x) - 9) })
VODIDs <- lapply(bucket1asVector, function(x) { str_sub(x, -9, -1) })
sentences = unlist(sentences)
VODIDs = unlist(VODIDs)

bucket1OriginalAsTibble <- tibble( text = sentences, book = as.factor(VODIDs))




bucket2asVector <- unlist(bucket2Original$yoooooooooooo732422168)
bucket2asVector <- iconv(enc2utf8(bucket2asVector), sub='byte')




sentences <- lapply(bucket2asVector, function(x) { str_sub(x, 1, nchar(x) - 9) })
VODIDs <- lapply(bucket2asVector, function(x) { str_sub(x, -9, -1) })
sentences = unlist(sentences)
VODIDs = unlist(VODIDs)

bucket2OriginalAsTibble <- tibble( text = sentences, book = as.factor(VODIDs))









#austen_bigrams <- austen_books() %>%
#  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams <- bucket1OriginalAsTibble %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams

austen_bigrams %>%
  count(bigram, sort = TRUE)



bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united

bucket1OriginalAsTibble %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)

bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)

bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)

AFINN <- get_sentiments("afinn")

AFINN

not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)

not_words



not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"")

negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)


# original counts
bigram_counts

# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

bigram_graph

set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

set.seed(2020)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()


count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}

# the King James version is book 10 on Project Gutenberg:

kjv <- gutenberg_download(10)


kjv_bigrams <- kjv %>%
  count_bigrams()

# filter out rare combinations, as well as digits
kjv_bigrams %>%
  filter(n > 40,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
  visualize_bigrams()

austen_section_words <- bucket1OriginalAsTibble %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word)

austen_section_words

# count words co-occuring within sections
word_pairs <- austen_section_words %>%
  pairwise_count(word, section, sort = TRUE)

word_pairs

word_pairs %>%
  filter(item1 == "darcy")

# we need to filter for at least relatively common words first
word_cors <- austen_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

word_cors

word_cors %>%
  filter(item1 == "pounds")

word_cors %>%
  filter(item1 %in% c("teep50", "lul", "kekw", "pog")) %>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()

set.seed(2016)

word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

















```

```{r}

























#austen_bigrams <- austen_books() %>%
#  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams <- bucket2OriginalAsTibble %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams

austen_bigrams %>%
  count(bigram, sort = TRUE)



bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united

bucket2OriginalAsTibble %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)

bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)

bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)

AFINN <- get_sentiments("afinn")

AFINN

not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)

not_words



not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"")

negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)


# original counts
bigram_counts

# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

bigram_graph

set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

set.seed(2020)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()


count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}

# the King James version is book 10 on Project Gutenberg:

kjv <- gutenberg_download(10)


kjv_bigrams <- kjv %>%
  count_bigrams()

# filter out rare combinations, as well as digits
kjv_bigrams %>%
  filter(n > 40,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
  visualize_bigrams()

austen_section_words <- bucket2OriginalAsTibble %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word)

austen_section_words

# count words co-occuring within sections
word_pairs <- austen_section_words %>%
  pairwise_count(word, section, sort = TRUE)

word_pairs

word_pairs %>%
  filter(item1 == "darcy")

# we need to filter for at least relatively common words first
word_cors <- austen_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

word_cors

word_cors %>%
  filter(item1 == "pounds")

word_cors %>%
  filter(item1 %in% c("teephype", "good", "just", "game")) %>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()

set.seed(2016)

word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```


```{r}


### re-paste the code
bucket1Original <- read_csv("bucket1Original.txt")

bucket2Original <- read_csv("bucket2Original.txt")


bucket1asVector <- unlist(bucket1Original$amateurs732422168)
bucket1asVector <- iconv(enc2utf8(bucket1asVector), sub='byte')



sentences <- lapply(bucket1asVector, function(x) { str_sub(x, 1, nchar(x) - 9) })
VODIDs <- lapply(bucket1asVector, function(x) { str_sub(x, -9, -1) })
sentences = unlist(sentences)
VODIDs = unlist(VODIDs)

bucket1OriginalAsTibble <- tibble( text = sentences, book = as.factor(VODIDs))




bucket2asVector <- unlist(bucket2Original$yoooooooooooo732422168)
bucket2asVector <- iconv(enc2utf8(bucket2asVector), sub='byte')




sentences <- lapply(bucket2asVector, function(x) { str_sub(x, 1, nchar(x) - 9) })
VODIDs <- lapply(bucket2asVector, function(x) { str_sub(x, -9, -1) })
sentences = unlist(sentences)
VODIDs = unlist(VODIDs)

bucket2OriginalAsTibble <- tibble( text = sentences, book = as.factor(VODIDs))
### end of re-pasted code


```


```{r}
bucket1Original <- read_csv("bucket1Original.txt")
bucket2Original <- read_csv("bucket2Original.txt")
```

```{r}
library(quanteda)
library(quanteda.textstats)



bucket1asVector <- unlist(bucket1Original$amateurs732422168)
bucket2asVector <- unlist(bucket2Original$yoooooooooooo732422168)
bucket1WithoutVODID <- gsub('.{9}$', '', bucket1asVector) # because nchar doesn't mesh with invalid multibyte strings
bucket2WithoutVODID <- gsub('.{9}$', '', bucket2asVector)


VODIDs1 <- lapply(bucket1asVector, function(x) { str_sub(x, -9, -1) })
VODIDs2 <- lapply(bucket2asVector, function(x) { str_sub(x, -9, -1) })
bucket1OriginalAsTibble <- tibble( text = bucket1WithoutVODID, vod = VODIDs1)
bucket2OriginalAsTibble <- tibble( text = bucket2WithoutVODID, vod = VODIDs2)

# filter out non-numeric glitches in the vod column
numbers_only <- function(x) !grepl("\\D", x)
bucket1OriginalAsTibble <- bucket1OriginalAsTibble[numbers_only(bucket1OriginalAsTibble$vod),]
bucket2OriginalAsTibble <- bucket2OriginalAsTibble[numbers_only(bucket2OriginalAsTibble$vod),]

# filter out vod id related glitches (they should be of length 9, right?)
lengthNineOnly <- function(x) nchar(x) == 9
bucket1OriginalAsTibble <- bucket1OriginalAsTibble[lengthNineOnly(bucket1OriginalAsTibble$vod),]
bucket2OriginalAsTibble <- bucket2OriginalAsTibble[lengthNineOnly(bucket2OriginalAsTibble$vod),]
```

```{r}
totalForHighlight <- ''
totalOtherwiseBucket2 <- ''
uniqueVODs <- unlist(unique(bucket1OriginalAsTibble$vod))

for (id in uniqueVODs) {
  res = 'VODIDONDEMAND'
  asdf <- bucket1OriginalAsTibble[bucket1OriginalAsTibble$vod == id,]
  text <- paste(asdf$text, collapse = ' ')
  text <- paste(c(res, text), collapse = ' ')
  totalForHighlight <- paste(totalForHighlight, text, collapse = ' ')
  
  asdf <- bucket2OriginalAsTibble[bucket2OriginalAsTibble$vod == id,]
  text <- paste(asdf$text, collapse = ' ')
  text <- paste(c(res, text), collapse = ' ')
  totalOtherwiseBucket2 <- paste(totalOtherwiseBucket2, text, collapse = ' ')
}
```


```{r}


library(readtext)
data_char_mobydick <- texts(readtext("http://www.gutenberg.org/cache/epub/2701/pg2701.txt"))
allTextDuringHighlights <- data_char_mobydick
allTextDuringTheSpaceBetweenHighlights <- data_char_mobydick


names(allTextDuringHighlights) <- "during highlight"
names(allTextDuringTheSpaceBetweenHighlights) <- "during the space between highlights"
allTextDuringHighlights[1] = totalForHighlight
allTextDuringTheSpaceBetweenHighlights[1] = totalOtherwiseBucket2



vod_corp <- 
    corpus(allTextDuringHighlights) %>%
    corpus_segment(pattern = "VODIDONDEMAND", valuetype = "regex")
summary(vod_corp, 10)
# create a dfm
vod_dfm <- dfm(vod_corp)

# extract row with count for "whale"/"ahab" in each chapter
# and convert to data frame for plotting
all_patterns_df_during_highlight <- vod_dfm %>% 
    dfm_keep(pattern = c("https", "subscribed", "catjam", "teephype", "good", "want", "close", "open")) %>% 
    convert(to = "data.frame")
    
all_patterns_df_during_highlight$vodid <- uniqueVODs

```

```{r}

ggplot(data = all_patterns_df_during_highlight, aes(x = vodid, y = https)) + 
  geom_bar(stat = "identity") +
  labs(x = "VOD", 
       y = "Frequency",
       title = paste('Occurrence of https')) + 
  theme(
    panel.background = element_rect(fill = "#fffaff",
                              colour = "lightblue",
                              size = 0.1, linetype = "solid"),
    panel.grid.minor = element_line(colour = "black", size = 0.25), 
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank()
  ) + 
  geom_text(aes(label = paste('#', vodid, sep='')))
ggsave("occurrenceHttpsHighlight.png")
ggplot(data = all_patterns_df_during_highlight, aes(x = vodid, y = subscribed)) + 
  geom_bar(stat = "identity") +
  labs(x = "VOD", 
       y = "Frequency",
       title = paste('Occurrence of subscribed')) + 
  theme(
    panel.background = element_rect(fill = "#fffaff",
                              colour = "lightblue",
                              size = 0.1, linetype = "solid"),
    panel.grid.minor = element_line(colour = "black", size = 0.25), 
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank()
  ) + 
  geom_text(aes(label = paste('#', vodid, sep='')))
ggsave("occurrenceSubscribedHighlight.png")
ggplot(data = all_patterns_df_during_highlight, aes(x = vodid, y = catjam)) + 
  geom_bar(stat = "identity") +
  labs(x = "VOD", 
       y = "Frequency",
       title = paste('Occurrence of catjam')) + 
  theme(
    panel.background = element_rect(fill = "#fffaff",
                              colour = "lightblue",
                              size = 0.1, linetype = "solid"),
    panel.grid.minor = element_line(colour = "black", size = 0.25), 
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank()
  ) + 
  geom_text(aes(label = paste('#', vodid, sep='')))
ggsave("occurrenceCatjamHighlight.png")
ggplot(data = all_patterns_df_during_highlight, aes(x = vodid, y = teephype)) + 
  geom_bar(stat = "identity") +
  labs(x = "VOD", 
       y = "Frequency",
       title = paste('Occurrence of teephype')) + 
  theme(
    panel.background = element_rect(fill = "#fffaff",
                              colour = "lightblue",
                              size = 0.1, linetype = "solid"),
    panel.grid.minor = element_line(colour = "black", size = 0.25), 
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank()
  ) + 
  geom_text(aes(label = paste('#', vodid, sep='')))
ggsave("occurrenceTeephypeHighlight.png")
ggplot(data = all_patterns_df_during_highlight, aes(x = vodid, y = good)) + 
  geom_bar(stat = "identity") +
  labs(x = "VOD", 
       y = "Frequency",
       title = paste('Occurrence of good')) + 
  theme(
    panel.background = element_rect(fill = "#fffaff",
                              colour = "lightblue",
                              size = 0.1, linetype = "solid"),
    panel.grid.minor = element_line(colour = "black", size = 0.25), 
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank()
  ) + 
  geom_text(aes(label = paste('#', vodid, sep='')))
ggsave("occurrenceGoodHighlight.png")
ggplot(data = all_patterns_df_during_highlight, aes(x = vodid, y = want)) + 
  geom_bar(stat = "identity") +
  labs(x = "VOD", 
       y = "Frequency",
       title = paste('Occurrence of want')) + 
  theme(
    panel.background = element_rect(fill = "#fffaff",
                              colour = "lightblue",
                              size = 0.1, linetype = "solid"),
    panel.grid.minor = element_line(colour = "black", size = 0.25), 
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank()
  ) + 
  geom_text(aes(label = paste('#', vodid, sep='')))
ggsave("occurrenceWantHighlight.png")


```


```{r}
dfm_weight(vod_dfm, scheme = "prop") %>% 
    textstat_simil(selection = c("close", "open"), method = "correlation", margin = "features") %>%
    as.matrix() %>%
    head(2)


cor_data_df <- dfm_weight(vod_dfm, scheme = "prop") %>% 
    dfm_keep(pattern = c("catjam", "teephype")) %>% 
    convert(to = "data.frame")

# sample 1000 replicates and create data frame
n <- 1000
samples <- data.frame(
    cor_sample = replicate(n, cor(sample(cor_data_df$teephype), cor_data_df$catjam)),
    id_sample = 1:n
)

# plot distribution of resampled correlations
ggplot(data = samples, aes(x = cor_sample, y = ..density..)) +
    geom_histogram(colour = "black", binwidth = 0.01, fill = 'snow') +
    geom_density(colour = "green") +
    labs(x = "Correlation Coefficient", y = NULL,
         title = "Histogram of Randomly Sampled Correlation Coefficients between 'close' and 'open'") + 
  theme(panel.background = element_blank())

ggsave("correlationCloseOpen.png")
```


```{r}
#allTextDuringTheSpaceBetweenHighlights

litcorpus <- corpus(c(allTextDuringHighlights))

(dogkwic <- kwic(litcorpus, pattern = "dog"))
textplot_xray(dogkwic, scale = "absolute")
(dogkwic <- kwic(litcorpus, pattern = "god"))
textplot_xray(dogkwic, scale = "absolute")
```




```{r}
library(SentimentAnalysis)
sentiment <- analyzeSentiment(unlist(bucket1OriginalAsTibble$text))
categorize <- convertToDirection(sentiment$SentimentQDAP)
barplot(categorize)


```


```{r}
ggplot(sentiment, aes(x = SentimentQDAP)) + 
    geom_histogram(position="dodge", binwidth = 0.01) +
  theme(
        panel.background = element_rect(size = 0.3, 
                                        linetype = 6, 
                                        fill = grDevices::rgb(100,90,94.118, 50, 
                                                              maxColorValue = 100)),
        panel.grid.major = element_line(size = 0.3, 
                                        linetype = 0,
                                        colour = "white"), 
        panel.grid.minor = element_line(size = 0.25, 
                                        linetype = 0,
                                        colour = "white"), 
        axis.ticks = element_blank())
```


```





